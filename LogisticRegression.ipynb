{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhvzWFHejZuLiSgf+rJit5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parvvaresh/Logistic-Regression-from-scratch/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Sigmoid Function (Logistic Function)**\n",
        "\n",
        "Logistic regression algorithm also uses a linear equation with independent predictors to predict a value. The predicted value can be anywhere between negative infinity to positive infinity. We need the output of the algorithm to be class variable, i.e 0-no, 1-yes. Therefore, we are squashing the output of the linear equation into a range of [0,1]. To squash the predicted value between 0 and 1, we use the sigmoid function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We take the output(z) of the linear equation and give to the function g(x) which returns a squashed value h, the value h will lie in the range of 0 to 1. To understand how sigmoid function squashes the values within the range, let’s visualize the graph of the sigmoid function.\n",
        "\n",
        "\n",
        "As you can see from the graph, the sigmoid function becomes asymptote to y=1 for positive values of x and becomes asymptote to y=0 for negative values of x.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/89921883/222824767-a82f2052-6efe-46fa-9c21-c001fc89e476.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Cost Function**\n",
        "Since we are trying to predict class values, we cannot use the same cost function used in linear regression algorithm. Therefore, we use a logarithmic loss function to calculate the cost for misclassifying.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/89921883/222825691-a3695da9-6cfa-4305-b38a-38a38c564b86.png)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Calculating Gradients**\n",
        "We take partial derivatives of the cost function with respect to each parameter(theta_0, theta_1, …) to obtain the gradients. with the help of these gradients, we can update the values of theta_0, theta_1, … To understand the equations below you would need some calculus.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/89921883/222826236-eb01297b-4943-4b9e-b7e1-ebac1d4eac34.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "aABmIg5LNmY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "\n",
        "  def __init__(self, learning_rate = 10e-5, n_iters = 10e3):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iters = n_iters\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def fit(self, x, y):\n",
        "\n",
        "    number_samples , number_features = x.shape\n",
        "    self.weights = np.zeros(number_features)\n",
        "    self.bias = 0\n",
        "    for repeat in range(0, self.n_iters):\n",
        "\n",
        "      y_predicted = self.sigmoid(np.dot(x, self.weights) + self.bias)\n",
        "      dw = (1 / number_samples) * (np.dot(x.T,  (y_predicted - y)))\n",
        "      db = (1 / number_samples) * (np.sum(y_predicted - y))\n",
        "\n",
        "      self.weights -= self.learning_rate * dw\n",
        "      self.bias -=  self.learning_rate * db\n",
        "\n",
        "  def predict(self, x):\n",
        "    \n",
        "    y_predicted = self.sigmoid(np.dot(x, self.weights) + self.bias)\n",
        "    y_predicted = [1 if element > 0.5 else 0 for element in y_predicted]\n",
        "    return np.array(y_predicted)\n",
        "\n",
        "  def accuracy(self, y, y_predicted):\n",
        "    return (np.sum(y == y_predicted) /len(y)) * 100"
      ],
      "metadata": {
        "id": "5lpPukAXNqNx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=1234\n",
        "    )\n",
        "\n",
        "model = LogisticRegression(learning_rate=0.0001, n_iters=10000)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(\"LR classification accuracy:\", model.accuracy(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9OlVdoeV4Nc",
        "outputId": "e234ce3d-155c-4e9b-aeb9-8d6e8bc634a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR classification accuracy: 90.35087719298247\n"
          ]
        }
      ]
    }
  ]
}